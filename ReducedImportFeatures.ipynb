{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributors: \n",
    "Kyle McCarver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import Image\n",
    "from time import sleep\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from amb_sdk.sdk import DarwinSdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Darwin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login\n",
    "ds = DarwinSdk()\n",
    "file = open(\"login.txt\", \"r\")\n",
    "username = file.readline(0)\n",
    "password = file.readline(1)\n",
    "ds.set_url('https://amb-demo-api.sparkcognition.com/v1/')\n",
    "status, msg = ds.auth_login_user('username', 'password')\n",
    "if not status:\n",
    "    print(msg)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Path\n",
    "Make sure to set this to your local machine's path to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data:\n",
    "Data used in this project:\n",
    "https://data.austintexas.gov/Building-and-Development/Issued-Construction-Permits/3syk-w9eu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (53,54,56,58,59,60,61,62,63,64,65,66) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Permit Type Desc', 'Permit Num', 'Permit Class Mapped', 'Permit Class', 'Condominium', 'Project Name', 'Description', 'TCAD ID', 'Property Legal Description', 'Applied Date', 'Issued Date', 'Day Issued', 'Issued In Last 30 Days', 'Issuance Method', 'Status Current', 'Status Date', 'Expires Date', 'Completed Date', 'Total Existing Bldg SQFT', 'Remodel Repair SQFT', 'Total New Add SQFT', 'Total Valuation Remodel', 'Building Valuation', 'Building Valuation Remodel', 'Electrical Valuation', 'Electrical Valuation Remodel', 'Mechanical Valuation', 'Mechanical Valuation Remodel', 'Plumbing Valuation', 'Plumbing Valuation Remodel', 'MedGas Valuation', 'MedGas Valuation Remodel', 'Original Address 1', 'Original City', 'Original State', 'Original Zip', 'Council District', 'Jurisdiction', 'Link', 'Project ID', 'Location', 'Contractor Trade', 'Contractor Company Name', 'Contractor Full Name', 'Contractor Phone', 'Contractor Address 1', 'Contractor Address 2', 'Contractor City', 'Contractor Zip', 'Applicant Full Name', 'Applicant Organization', 'Applicant Phone', 'Applicant Address 1', 'Applicant Address 2', 'Applicant City', 'Applicant Zip', 'Certificate Of Occupancy', 'Total Lot SQFT']\n",
      "Removed columns from dataset:\n",
      "Contractor Company Name\n",
      "Contractor Phone\n",
      "Contractor Address 2\n",
      "Contractor Zip\n",
      "Applicant Full Name\n",
      "Applicant Organization\n",
      "Applicant Phone\n",
      "Applicant Address 1\n",
      "Applicant Address 2\n",
      "Applicant City\n",
      "Applicant Zip\n",
      "Certificate Of Occupancy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Permit Type</th>\n",
       "      <th>Work Class</th>\n",
       "      <th>Calendar Year Issued</th>\n",
       "      <th>Fiscal Year Issued</th>\n",
       "      <th>Total Job Valuation</th>\n",
       "      <th>Number Of Floors</th>\n",
       "      <th>Housing Units</th>\n",
       "      <th>Master Permit Num</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1046582</th>\n",
       "      <td>PP</td>\n",
       "      <td>Remodel</td>\n",
       "      <td>1985</td>\n",
       "      <td>1985</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>555854.0</td>\n",
       "      <td>30.371660</td>\n",
       "      <td>-97.729674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173939</th>\n",
       "      <td>PP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2002</td>\n",
       "      <td>2003</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.178747</td>\n",
       "      <td>-97.757743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099587</th>\n",
       "      <td>PP</td>\n",
       "      <td>Remodel</td>\n",
       "      <td>1992</td>\n",
       "      <td>1992</td>\n",
       "      <td>3100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3300364.0</td>\n",
       "      <td>30.289870</td>\n",
       "      <td>-97.722097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99612</th>\n",
       "      <td>EP</td>\n",
       "      <td>Remodel</td>\n",
       "      <td>2010</td>\n",
       "      <td>2011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.342565</td>\n",
       "      <td>-97.757923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1890364</th>\n",
       "      <td>BP</td>\n",
       "      <td>Remodel</td>\n",
       "      <td>2006</td>\n",
       "      <td>2007</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.265644</td>\n",
       "      <td>-97.780056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Permit Type Work Class  Calendar Year Issued  Fiscal Year Issued  \\\n",
       "1046582          PP    Remodel                  1985                1985   \n",
       "1173939          PP        NaN                  2002                2003   \n",
       "1099587          PP    Remodel                  1992                1992   \n",
       "99612            EP    Remodel                  2010                2011   \n",
       "1890364          BP    Remodel                  2006                2007   \n",
       "\n",
       "         Total Job Valuation  Number Of Floors  Housing Units  \\\n",
       "1046582               2000.0               NaN            1.0   \n",
       "1173939              60000.0               NaN            NaN   \n",
       "1099587               3100.0               NaN            1.0   \n",
       "99612                    NaN               0.0            0.0   \n",
       "1890364               2000.0               NaN            NaN   \n",
       "\n",
       "         Master Permit Num   Latitude  Longitude  \n",
       "1046582           555854.0  30.371660 -97.729674  \n",
       "1173939                NaN  30.178747 -97.757743  \n",
       "1099587          3300364.0  30.289870 -97.722097  \n",
       "99612                  NaN  30.342565 -97.757923  \n",
       "1890364                NaN  30.265644 -97.780056  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFile = \"./Issued_Construction_Permits.csv\"\n",
    "filename= \"train.csv\"\n",
    "test = 'test.csv'\n",
    "data = pd.read_csv(dataFile, skipinitialspace=True)\n",
    "\n",
    "#Columns with detected mixed types\n",
    "mixedData_col = [52,54,56,58,59,60,61,62,63,64,65,66]\n",
    "\n",
    "columnsNames = data.columns.values\n",
    "#excess labels included in feature drop\n",
    "#Darwin doesn't like mulitple date fields either so they must be dropped as well\n",
    "\n",
    "'''\n",
    "Master Permit Num       0.116584\n",
    "Housing Units           0.039151\n",
    "Total Job Valuation     0.027820\n",
    "Number Of Floors        0.025094\n",
    "Fiscal Year Issued      0.016996\n",
    "Calendar Year Issued    0.016561\n",
    "Longitude               0.014342\n",
    "Latitude                0.013554\n",
    "Work Class = New        0.012211'''\n",
    "important_features = [\"Permit Type\", \"Master Permit Num\", \"Housing Units\", \"Total Job Valuation\", \"Number Of Floors\", \n",
    "                      \"Fiscal Year Issued\", \"Calendar Year Issued\", \"Longitude\", \"Latitude\", \"Work Class\"]\n",
    "featureDrop = [x for x in columnsNames if x not in important_features]\n",
    "print(featureDrop)\n",
    "\n",
    "#add mixed datatype col to feature drop until later date of processing\n",
    "print(\"Removed columns from dataset:\")\n",
    "for col in mixedData_col:\n",
    "    print(columnsNames[col])\n",
    "    featureDrop.append(columnsNames[col])\n",
    "    \n",
    "fullData = data.drop(featureDrop, axis=1)\n",
    "#data added chronologically to dataset, for now reduce by half for random sampling\n",
    "#pick sample sizes (max is half the dataset due to Darwin restrictions on Big Data)\n",
    "trainSize = math.floor(len(fullData)/2)\n",
    "testSize = math.floor(len(fullData)/10)\n",
    "\n",
    "#sample train and test sets, note currently using .sample() performs without replacement on each instance, meaning\n",
    "#there might exist overlap between the two sets\n",
    "testSet = fullData.sample(n=testSize)\n",
    "trainData = fullData.sample(n=trainSize)\n",
    "\n",
    "#write out datasets to disk to upload later\n",
    "testSet.to_csv(os.path.join(path, test))\n",
    "trainData.to_csv(os.path.join(path, filename))\n",
    "\n",
    "#show data / completed write to disk\n",
    "trainData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset we will attempt to use the Permit Type as the class label for the data and because of that we need to remove some columns that also act as labels in that capacity that might skew the results. Columns that are similar to Permit type are:\n",
    "\n",
    "    Permit Type Description (too hard to process due to variablility of descriptions, but also typically contains label name)\n",
    "    Permit Num (since it contain the type in the code)\n",
    "    Permit Class (functions much like a label)\n",
    "    Contractor Trade (plummers typically take plumbing jobs, electricians take electrician jobs thus could act as a label)\n",
    "    Project ID (applies to series of permits stored in folder in database, unsurprisingly permits are put in similar folders)\n",
    "\n",
    "Tangental Job indicators: (subject to testing and feature engineering)\n",
    "\n",
    "    *Plumbing Valuation\n",
    "    *Plumbing Valuation Remodel\n",
    "    *Electrical Valuation\n",
    "    *Electrical Valuation Remodel\n",
    "    *Mechanical Valuation\n",
    "    *Mechanical Valuation Remodel \n",
    "    *MedGas Valuation\n",
    "    *MedGas Valuation Remodel\n",
    "\n",
    "It might be interesting to note that a renovation job might include plumbing costs and the difference being the cost threshold which decides if its specifically a plumbing job.\n",
    "\n",
    "Interesting sidenote our data qualifies as big data under Darwin's algorithms (>500MB in size) as such not only can we sample\n",
    "at most half our dataset but also the need to remove addional columns before uploading including those above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload to Darwin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, dataset = ds.upload_dataset(os.path.join(path, filename))\n",
    "if not status:\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'job_name': 'c590a799e1904cc096b69b5b2ca5b306', 'artifact_name': 'b004e3e2bbb644c0ae040001d30f76a0'}\n",
      "{'status': 'Requested', 'starttime': '2019-04-22T20:56:06.139236', 'endtime': None, 'percent_complete': 0, 'job_type': 'CleanDataTiny', 'loss': None, 'generations': None, 'dataset_names': ['train.csv'], 'artifact_names': ['b004e3e2bbb644c0ae040001d30f76a0'], 'model_name': None, 'job_error': None}\n",
      "{'status': 'Running', 'starttime': '2019-04-22T20:56:06.139236', 'endtime': None, 'percent_complete': 0, 'job_type': 'CleanDataTiny', 'loss': None, 'generations': None, 'dataset_names': ['train.csv'], 'artifact_names': ['b004e3e2bbb644c0ae040001d30f76a0'], 'model_name': None, 'job_error': ''}\n",
      "{'status': 'Running', 'starttime': '2019-04-22T20:56:06.139236', 'endtime': None, 'percent_complete': 0, 'job_type': 'CleanDataTiny', 'loss': None, 'generations': None, 'dataset_names': ['train.csv'], 'artifact_names': ['b004e3e2bbb644c0ae040001d30f76a0'], 'model_name': None, 'job_error': ''}\n",
      "{'status': 'Running', 'starttime': '2019-04-22T20:56:06.139236', 'endtime': None, 'percent_complete': 0, 'job_type': 'CleanDataTiny', 'loss': None, 'generations': None, 'dataset_names': ['train.csv'], 'artifact_names': ['b004e3e2bbb644c0ae040001d30f76a0'], 'model_name': None, 'job_error': ''}\n",
      "{'status': 'Running', 'starttime': '2019-04-22T20:56:06.139236', 'endtime': None, 'percent_complete': 0, 'job_type': 'CleanDataTiny', 'loss': None, 'generations': None, 'dataset_names': ['train.csv'], 'artifact_names': ['b004e3e2bbb644c0ae040001d30f76a0'], 'model_name': None, 'job_error': ''}\n",
      "{'status': 'Running', 'starttime': '2019-04-22T20:56:06.139236', 'endtime': None, 'percent_complete': 0, 'job_type': 'CleanDataTiny', 'loss': None, 'generations': None, 'dataset_names': ['train.csv'], 'artifact_names': ['b004e3e2bbb644c0ae040001d30f76a0'], 'model_name': None, 'job_error': ''}\n",
      "{'status': 'Running', 'starttime': '2019-04-22T20:56:06.139236', 'endtime': None, 'percent_complete': 0, 'job_type': 'CleanDataTiny', 'loss': None, 'generations': None, 'dataset_names': ['train.csv'], 'artifact_names': ['b004e3e2bbb644c0ae040001d30f76a0'], 'model_name': None, 'job_error': ''}\n",
      "{'status': 'Running', 'starttime': '2019-04-22T20:56:06.139236', 'endtime': None, 'percent_complete': 0, 'job_type': 'CleanDataTiny', 'loss': None, 'generations': None, 'dataset_names': ['train.csv'], 'artifact_names': ['b004e3e2bbb644c0ae040001d30f76a0'], 'model_name': None, 'job_error': ''}\n",
      "{'status': 'Complete', 'starttime': '2019-04-22T20:56:06.139236', 'endtime': '2019-04-22T20:57:57.839087', 'percent_complete': 100, 'job_type': 'CleanDataTiny', 'loss': None, 'generations': None, 'dataset_names': ['train.csv'], 'artifact_names': ['b004e3e2bbb644c0ae040001d30f76a0'], 'model_name': None, 'job_error': ''}\n"
     ]
    }
   ],
   "source": [
    "# clean dataset\n",
    "target = \"Permit Type\"\n",
    "index = \"Applied Date\"\n",
    "status, job_id = ds.clean_data(filename, target = target)\n",
    "print(job_id)\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'Complete', 'starttime': '2019-04-22T20:56:06.139236', 'endtime': '2019-04-22T20:57:57.839087', 'percent_complete': 100, 'job_type': 'CleanDataTiny', 'loss': None, 'generations': None, 'dataset_names': ['train.csv'], 'artifact_names': ['b004e3e2bbb644c0ae040001d30f76a0'], 'model_name': None, 'job_error': ''}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, 'Job completed')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.wait_for_job(job_id['job_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'Requested', 'starttime': '2019-04-22T21:00:57.817859', 'endtime': None, 'percent_complete': 0, 'job_type': 'TrainModel', 'loss': None, 'generations': 0, 'dataset_names': ['train.csv'], 'artifact_names': None, 'model_name': 'Permit Type_model', 'job_error': None}\n",
      "{'status': 'Requested', 'starttime': '2019-04-22T21:00:57.817859', 'endtime': None, 'percent_complete': 0, 'job_type': 'TrainModel', 'loss': None, 'generations': 0, 'dataset_names': ['train.csv'], 'artifact_names': None, 'model_name': 'Permit Type_model', 'job_error': None}\n",
      "{'status': 'Requested', 'starttime': '2019-04-22T21:00:57.817859', 'endtime': None, 'percent_complete': 0, 'job_type': 'TrainModel', 'loss': None, 'generations': 0, 'dataset_names': ['train.csv'], 'artifact_names': None, 'model_name': 'Permit Type_model', 'job_error': None}\n",
      "{'status': 'Requested', 'starttime': '2019-04-22T21:00:57.817859', 'endtime': None, 'percent_complete': 0, 'job_type': 'TrainModel', 'loss': None, 'generations': 0, 'dataset_names': ['train.csv'], 'artifact_names': None, 'model_name': 'Permit Type_model', 'job_error': None}\n",
      "{'status': 'Requested', 'starttime': '2019-04-22T21:00:57.817859', 'endtime': None, 'percent_complete': 0, 'job_type': 'TrainModel', 'loss': None, 'generations': 0, 'dataset_names': ['train.csv'], 'artifact_names': None, 'model_name': 'Permit Type_model', 'job_error': None}\n",
      "{'status': 'Requested', 'starttime': '2019-04-22T21:00:57.817859', 'endtime': None, 'percent_complete': 0, 'job_type': 'TrainModel', 'loss': None, 'generations': 0, 'dataset_names': ['train.csv'], 'artifact_names': None, 'model_name': 'Permit Type_model', 'job_error': None}\n",
      "{'status': 'Requested', 'starttime': '2019-04-22T21:00:57.817859', 'endtime': None, 'percent_complete': 0, 'job_type': 'TrainModel', 'loss': None, 'generations': 0, 'dataset_names': ['train.csv'], 'artifact_names': None, 'model_name': 'Permit Type_model', 'job_error': None}\n",
      "{'status': 'Requested', 'starttime': '2019-04-22T21:00:57.817859', 'endtime': None, 'percent_complete': 0, 'job_type': 'TrainModel', 'loss': None, 'generations': 0, 'dataset_names': ['train.csv'], 'artifact_names': None, 'model_name': 'Permit Type_model', 'job_error': None}\n",
      "{'status': 'Requested', 'starttime': '2019-04-22T21:00:57.817859', 'endtime': None, 'percent_complete': 0, 'job_type': 'TrainModel', 'loss': None, 'generations': 0, 'dataset_names': ['train.csv'], 'artifact_names': None, 'model_name': 'Permit Type_model', 'job_error': None}\n",
      "{'status': 'Requested', 'starttime': '2019-04-22T21:00:57.817859', 'endtime': None, 'percent_complete': 0, 'job_type': 'TrainModel', 'loss': None, 'generations': 0, 'dataset_names': ['train.csv'], 'artifact_names': None, 'model_name': 'Permit Type_model', 'job_error': None}\n",
      "{'status': 'Running', 'starttime': '2019-04-22T21:00:57.817859', 'endtime': None, 'percent_complete': 0, 'job_type': 'TrainModel', 'loss': None, 'generations': 0, 'dataset_names': ['train.csv'], 'artifact_names': None, 'model_name': 'Permit Type_model', 'job_error': ''}\n",
      "{'status': 'Running', 'starttime': '2019-04-22T21:00:57.817859', 'endtime': None, 'percent_complete': 0, 'job_type': 'TrainModel', 'loss': None, 'generations': 0, 'dataset_names': ['train.csv'], 'artifact_names': None, 'model_name': 'Permit Type_model', 'job_error': ''}\n",
      "{'status': 'Running', 'starttime': '2019-04-22T21:00:57.817859', 'endtime': None, 'percent_complete': 0, 'job_type': 'TrainModel', 'loss': None, 'generations': 0, 'dataset_names': ['train.csv'], 'artifact_names': None, 'model_name': 'Permit Type_model', 'job_error': ''}\n",
      "{'status': 'Running', 'starttime': '2019-04-22T21:00:57.817859', 'endtime': None, 'percent_complete': 0, 'job_type': 'TrainModel', 'loss': None, 'generations': 0, 'dataset_names': ['train.csv'], 'artifact_names': None, 'model_name': 'Permit Type_model', 'job_error': ''}\n",
      "{'status': 'Running', 'starttime': '2019-04-22T21:00:57.817859', 'endtime': None, 'percent_complete': 0, 'job_type': 'TrainModel', 'loss': None, 'generations': 0, 'dataset_names': ['train.csv'], 'artifact_names': None, 'model_name': 'Permit Type_model', 'job_error': ''}\n",
      "{'status': 'Running', 'starttime': '2019-04-22T21:00:57.817859', 'endtime': None, 'percent_complete': 0, 'job_type': 'TrainModel', 'loss': None, 'generations': 0, 'dataset_names': ['train.csv'], 'artifact_names': None, 'model_name': 'Permit Type_model', 'job_error': ''}\n",
      "{'status': 'Running', 'starttime': '2019-04-22T21:00:57.817859', 'endtime': None, 'percent_complete': 0, 'job_type': 'TrainModel', 'loss': None, 'generations': 0, 'dataset_names': ['train.csv'], 'artifact_names': None, 'model_name': 'Permit Type_model', 'job_error': ''}\n",
      "{'status': 'Running', 'starttime': '2019-04-22T21:00:57.817859', 'endtime': None, 'percent_complete': 0, 'job_type': 'TrainModel', 'loss': None, 'generations': 0, 'dataset_names': ['train.csv'], 'artifact_names': None, 'model_name': 'Permit Type_model', 'job_error': ''}\n",
      "{'status': 'Running', 'starttime': '2019-04-22T21:00:57.817859', 'endtime': None, 'percent_complete': 0, 'job_type': 'TrainModel', 'loss': None, 'generations': 0, 'dataset_names': ['train.csv'], 'artifact_names': None, 'model_name': 'Permit Type_model', 'job_error': ''}\n",
      "{'status': 'Running', 'starttime': '2019-04-22T21:00:57.817859', 'endtime': None, 'percent_complete': 0, 'job_type': 'TrainModel', 'loss': None, 'generations': 0, 'dataset_names': ['train.csv'], 'artifact_names': None, 'model_name': 'Permit Type_model', 'job_error': ''}\n",
      "{'status': 'Running', 'starttime': '2019-04-22T21:00:57.817859', 'endtime': None, 'percent_complete': 0, 'job_type': 'TrainModel', 'loss': None, 'generations': 0, 'dataset_names': ['train.csv'], 'artifact_names': None, 'model_name': 'Permit Type_model', 'job_error': ''}\n",
      "{'status': 'Running', 'starttime': '2019-04-22T21:00:57.817859', 'endtime': None, 'percent_complete': 0, 'job_type': 'TrainModel', 'loss': None, 'generations': 0, 'dataset_names': ['train.csv'], 'artifact_names': None, 'model_name': 'Permit Type_model', 'job_error': ''}\n",
      "{'status': 'Running', 'starttime': '2019-04-22T21:00:57.817859', 'endtime': None, 'percent_complete': 0, 'job_type': 'TrainModel', 'loss': None, 'generations': 0, 'dataset_names': ['train.csv'], 'artifact_names': None, 'model_name': 'Permit Type_model', 'job_error': ''}\n",
      "{'status': 'Running', 'starttime': '2019-04-22T21:00:57.817859', 'endtime': None, 'percent_complete': 0, 'job_type': 'TrainModel', 'loss': None, 'generations': 0, 'dataset_names': ['train.csv'], 'artifact_names': None, 'model_name': 'Permit Type_model', 'job_error': ''}\n",
      "{'status': 'Running', 'starttime': '2019-04-22T21:00:57.817859', 'endtime': None, 'percent_complete': 0, 'job_type': 'TrainModel', 'loss': None, 'generations': 0, 'dataset_names': ['train.csv'], 'artifact_names': None, 'model_name': 'Permit Type_model', 'job_error': ''}\n",
      "{'status': 'Failed', 'starttime': '2019-04-22T21:00:57.817859', 'endtime': '2019-04-22T21:07:08.444792', 'percent_complete': 100, 'job_type': 'TrainModel', 'loss': None, 'generations': 0, 'dataset_names': ['train.csv'], 'artifact_names': None, 'model_name': 'Permit Type_model', 'job_error': 'DarwinInternalError: uncaught'}\n"
     ]
    }
   ],
   "source": [
    "model = target + \"_model\"\n",
    "status, job_id = ds.create_model(dataset_names = filename, \\\n",
    "                                 model_name =  model, \\\n",
    "                                 max_train_time = '00:01', \\\n",
    "                                 max_epochs=0)\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check status of job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'Failed', 'starttime': '2019-04-22T21:00:57.817859', 'endtime': '2019-04-22T21:07:08.444792', 'percent_complete': 100, 'job_type': 'TrainModel', 'loss': None, 'generations': 0, 'dataset_names': ['train.csv'], 'artifact_names': None, 'model_name': 'Permit Type_model', 'job_error': 'DarwinInternalError: uncaught'}\n"
     ]
    }
   ],
   "source": [
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404: NOT FOUND - {\"message\": \"Failed to find a dataset that the model was trained on. You have requested this URI [/v1/analyze/model/Permit Type_model] but did you mean /v1/analyze/model/<model_name> or /v1/analyze/model/predictions/<model_name>/<dataset_name> ?\"}\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9feec7a755ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martifact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_importance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martifact\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'artifact_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "# Retrieve feature importance of built model\n",
    "status, artifact = ds.analyze_model(model)\n",
    "sleep(1)\n",
    "if status:\n",
    "    ds.wait_for_job(artifact['job_name'])\n",
    "else:\n",
    "    print(artifact)\n",
    "status, feature_importance = ds.download_artifact(artifact['artifact_name'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display most important features of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "\n",
    "#### Perform model prediction on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(filename, model)\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download predictions from Darwin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "prediction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.to_csv(os.path.join(path, \"prediction10.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create visualizations for comparing predictions with actual target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(reduceData[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(classification_report(reduceData[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform model prediction on test dataset from holdout method.\n",
    "Upload test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "status, dataset = ds.upload_dataset(os.path.join(path, test_data))\n",
    "if not status:\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, job_id = ds.clean_data(test_data, target = target, model_name = model)\n",
    "print(\"Model:\\n\",model)\n",
    "print(\"Target: \\n\",target)\n",
    "print(job_id)\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run model on test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(test_data, model)\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create visualizations for comparing predictions with actual target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "df = pd.read_csv(os.path.join(path,test_data))\n",
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(df[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(classification_report(df[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Darwin' Pick for machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, model_type = ds.lookup_model_name(model)\n",
    "print(model_type['description']['best_genome'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting Permit Type_model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ds.delete_all_datasets()\n",
    "ds.delete_all_models()\n",
    "#ds.delete_all_artifacts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
