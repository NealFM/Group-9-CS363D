{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import Image\n",
    "from time import sleep\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from amb_sdk.sdk import DarwinSdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Darwin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Login\n",
    "ds = DarwinSdk()\n",
    "file = open(\"login.txt\", \"r\")\n",
    "username = file.readline(0)\n",
    "password = file.readline(1)\n",
    "ds.set_url('https://amb-demo-api.sparkcognition.com/v1/')\n",
    "status, msg = ds.auth_login_user('username', 'password')\n",
    "if not status:\n",
    "    print(msg)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Path\n",
    "Make sure to set this to your local machine's path to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data:\n",
    "Data used in this project:\n",
    "https://data.austintexas.gov/Building-and-Development/Issued-Construction-Permits/3syk-w9eu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contractor Company Name\n",
      "Contractor Phone\n",
      "Contractor Address 2\n",
      "Contractor Zip\n",
      "Applicant Full Name\n",
      "Applicant Organization\n",
      "Applicant Phone\n",
      "Applicant Address 1\n",
      "Applicant Address 2\n",
      "Applicant City\n",
      "Applicant Zip\n",
      "Certificate Of Occupancy\n"
     ]
    }
   ],
   "source": [
    "filename = \"Issued_Construction_Permits.csv\"\n",
    "#data = pd.read_csv(filename, skipinitialspace=True)\n",
    "\n",
    "#data.head()\n",
    "labels = data.iloc[:,0]\n",
    "raw_data = data.drop(['Permit Type Desc', 'Permit Num', 'Permit Class', 'Contractor Trade'], axis=1)\n",
    "\n",
    "raw_data.head()\n",
    "labels.head()\n",
    "\n",
    "#Columns with detected mixed types\n",
    "#mixed_data_col = [52,54,56,58,59,60,61,62,63,64,65,66]\n",
    "\n",
    "#columnsNames = data.columns.values\n",
    "#for col in mixed_data_col:\n",
    "#    print(columnsNames[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset we will attempt to use the Permit Type as the class label for the data and because of that we need to remove some columns that also act as labels in that capacity that might skew the results. Columns that are similar to Permit type are:\n",
    "\n",
    "\n",
    "Permit Type Description\n",
    "\n",
    "Permit Num (since it contain the type in the code)\n",
    "\n",
    "Permit Class (functions much like a label)\n",
    "\n",
    "Contractor Trade (plummers typically take plumbing jobs, electricians take electrician jobs thus could act as a label)\n",
    "\n",
    "Tangental Job indicators: (subject to testing and feature engineering)\n",
    "\n",
    "    *Plumbing Valuation\n",
    "    *Plumbing Valuation Remodel\n",
    "    *Electrical Valuation\n",
    "    *Electrical Valuation Remodel\n",
    "    *Mechanical Valuation\n",
    "    *Mechanical Valuation Remodel \n",
    "    *MedGas Valuation\n",
    "    *MedGas Valuation Remodel\n",
    "\n",
    "It might be interesting to note that a renovation job might include plumbing costs and the difference being the cost threshold which decides if its specifically a plumbing job.\n",
    "\n",
    "Mixed data types dectected by Pandas (need to cleaned prior to sending to Darwin)\n",
    "    \n",
    "    \n",
    "    Contractor Company Name (string)\n",
    "    Contractor Phone (convert to int?)\n",
    "    Contractor Address 2 (String)\n",
    "    Contractor Zip (int?)\n",
    "    Applicant Full Name (String)\n",
    "    Applicant Organization (String)\n",
    "    Applicant Phone (int?)\n",
    "    Applicant Address 1 (String)\n",
    "    Applicant Address 2 (String)\n",
    "    Applicant City (String, though all for Austin)\n",
    "    Applicant Zip (int?)\n",
    "    Certificate Of Occupancy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload to Darwin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "status, dataset = ds.upload_dataset(os.path.join(path, filename))\n",
    "if not status:\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean dataset\n",
    "target = \"Permit Type\"\n",
    "status, job_id = ds.clean_data(raw_data, target = target)\n",
    "\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = target + \"_model0\"\n",
    "status, job_id = ds.create_model(dataset_names = dataset_name, \\\n",
    "                                 model_name =  model, \\\n",
    "                                 max_train_time = '00:30')\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Retrieve feature importance of built model\n",
    "status, artifact = ds.analyze_model(model)\n",
    "sleep(1)\n",
    "if status:\n",
    "    ds.wait_for_job(artifact['job_name'])\n",
    "else:\n",
    "    print(artifact)\n",
    "status, feature_importance = ds.download_artifact(artifact['artifact_name'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display most important features of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_importance[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "\n",
    "#### Perform model prediction on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(dataset_name, model)\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download predictions from Darwin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "prediction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create visualizations for comparing predictions with actual target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(df[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(classification_report(df[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform model prediction on test dataset from holdout method.\n",
    "Upload test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = 'Permits_test.csv'\n",
    "status, dataset = ds.upload_dataset(os.path.join(path, test_data))\n",
    "if not status:\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "status, job_id = ds.clean_data(test_data, target = target, model_name = model)\n",
    "\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run model on test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(test_data, model)\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create visualizations for comparing predictions with actual target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "df = pd.read_csv(os.path.join(path,test_data))\n",
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(df[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(classification_report(df[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Darwin' Pick for machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "status, model_type = ds.lookup_model_name(model)\n",
    "print(model_type['description']['best'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
